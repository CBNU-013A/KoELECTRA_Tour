{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset for KoELECTRA, KoBERT NER Fine-tuning\n",
    "\n",
    "# 1. Introduction\n",
    "\n",
    "1. Few-shot을 위한 few examples 작성\n",
    "2. LLM을 이용하여 리스트 확장\n",
    "3. LLM을 이용하여 문장 생성\n",
    "4. LLM을 이용하여 BIO 태깅\n",
    "5. Train/Test set split & processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Few-shot을 위한 few examples \n",
    "    -> 01_fewshot.json\n",
    "\n",
    "# 3. LLM을 이용한 리스트 확장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 openAI API 설정\n",
    "TODO: Local LLM을 쓸때는 어캐함?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import ollama\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = ollama.Client(\n",
    "    host='http://localhost:11434'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 프롬포트 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_expend_prompt(prompts, model=\"gpt-4o\", max_tokens=1024):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompts}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_completion_tokens=max_tokens,\n",
    "        )\n",
    "        texts = [choice.message.content.strip() for choice in response.choices]\n",
    "        return texts\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI API 호출 중 오류 발생: {e}\")\n",
    "        return []\n",
    "\n",
    "def expend_prompt(class_name, entity_names, k=100):\n",
    "    prompt = f\"Below is a list of <{class_name}> entity names in Korean. Please list exactly {k} new <{class_name}> entity names in Korean that are similar.\\n\\n\"\n",
    "    prompt += \"Existing entity names:\\n\"\n",
    "    for e in entity_names:\n",
    "        prompt += f\"- {e}\\n\"\n",
    "    prompt += \"\\nNew entity names:\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 post processing\n",
    "\n",
    "`clean_text()` : 문자열에서 유니코드 특수문자 및 불필요 공백 제거\n",
    "`postprocess_entities()`: API응답에서 개체명만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text):\n",
    "    return unicodedata.normalize(\"NFKC\", text).encode(\"utf-8\", \"ignore\").decode(\"utf-8\").strip()\n",
    "\n",
    "def postprocess_entities(synthetic_entities):\n",
    "    processed = []\n",
    "    for ents in synthetic_entities:\n",
    "        # 응답 전체에서 줄 단위로 분할\n",
    "        lines = ents.split(\"\\n\")\n",
    "        new_entities = []\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # 숫자로 시작하는 항목만 처리 (예: \"1. 서대문\" 또는 \"2) 경복궁\")\n",
    "            if re.match(r'^\\d+[\\.\\)]', line):\n",
    "                # 숫자와 구분 기호 제거\n",
    "                line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line)\n",
    "                line = line.replace(\"-\", \"\").strip()\n",
    "                line = unicodedata.normalize(\"NFKC\", line).encode(\"utf-8\", \"ignore\").decode(\"utf-8\").strip()\n",
    "                if line:\n",
    "                    new_entities.append(line)\n",
    "        processed += new_entities\n",
    "    # 중복 제거 후 반환\n",
    "    return list(set(processed))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Load JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"01_fewshot.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    few_entities = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 개체명 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:51<00:00, 12.96s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "expended_entities = []\n",
    "\n",
    "for real_ent in tqdm(few_entities):\n",
    "    class_name, entity_names = real_ent['class_name'], real_ent['entity_name']\n",
    "    prompt = expend_prompt(class_name, entity_names)\n",
    "    syn_entities = generate_expend_prompt(prompt)\n",
    "    syn_entities = postprocess_entities(syn_entities)\n",
    "    expended_entities.append({'class_name': class_name, 'entity_name': syn_entities})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 data 저장\n",
    "Data 확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"02_expended_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(expended_entities, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LLM을 확용하여 문장 생성\n",
    "\n",
    "## 4.1. sampling\n",
    "전체 개체명 데이터셋에서 랜덤하게 min_k이상 max_k 이하의 개체명을 샘플링\n",
    "(현재는 class가 2개뿐이지만, 혹시모르니 일단 진행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_entities(all_entities, min_k=1, max_k=4):\n",
    "    k = np.random.randint(min_k, max_k+1)\n",
    "    idxs = np.random.choice(range(len(all_entities)), size=k, replace=False)\n",
    "    entities = []\n",
    "    for i in idxs:\n",
    "        ents = all_entities[i]\n",
    "        name = np.random.choice(ents[\"entity_name\"])\n",
    "        entities.append({\"class_name\": ents[\"class_name\"], \"entity_name\": name})\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 프롬포트 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_prompt(prompts, model=\"gpt-4o\", max_tokens=512):\n",
    "    responses = []\n",
    "    for prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_completion_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        generated_text = response.choices[0].message.content.strip()\n",
    "        responses.append(generated_text)\n",
    "    return responses\n",
    "\n",
    "def generate_sentence_prompt(entities, style=\"dialog\"):\n",
    "    prompt = f\"Generate a {style} sentence that includes the following entities in Korean.\\n\\n\"\n",
    "    entities_string = \", \".join([f\"{e['entity_name']}({e['class_name']})\" for e in entities])\n",
    "    prompt += f\"Entities: {entities_string}\\n\"\n",
    "    prompt += \"Sentence:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 post processing\n",
    "\n",
    "- 문장에서 \"삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.strip('\"')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Data Load\n",
    "\n",
    "3번단계 API호출 안하고 넘어가기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"02_expended_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    expended_entities = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 진행\n",
    "\n",
    "`num_iteration`에 반복할 횟수 설정\n",
    "`batch_size`에 API 한번에 생성할 문장수 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:59<00:00,  6.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "num_iterations = 100\n",
    "batch_size = 5\n",
    "\n",
    "generated_sentences = []\n",
    "\n",
    "for _ in tqdm(range(num_iterations)):\n",
    "    batch_entities = [sample_entities(expended_entities) for _ in range(batch_size)]\n",
    "    batch_prompts = [generate_sentence_prompt(ents) for ents in batch_entities]\n",
    "    batch_generated = generate_dataset_prompt(batch_prompts, model=\"gpt-4o\", max_tokens=256)\n",
    "    for generated, entities in zip(batch_generated, batch_entities):\n",
    "        cleaned_sentence = clean_sentence(generated)\n",
    "        generated_sentences.append({\"entities\": entities, \"sentence\": cleaned_sentence})\n",
    "    time.sleep(1)  # rate limit 방지를 위한 딜레이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 저장\n",
    "Data 확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"03_generated_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(generated_sentences, f, ensure_ascii=False, indent=4)\n",
    "#print(json.dumps(data, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LLM을 활용하여 BIO 태깅\n",
    "\n",
    "## 5.1. 프롬포트 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bio_prompt(prompts, model=\"gpt-4o\", max_tokens=512):\n",
    "    dev_msg = (\n",
    "        \"You are a helpful assistant.\\n\"\n",
    "        \"DO NOT INCLUDE OTHER COMMNENTS IN THE OUTPUT.\"\n",
    "        )\n",
    "    responses = []\n",
    "    for prompt in prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"developer\", \"content\": dev_msg},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_completion_tokens=max_tokens,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        generated_text = response.choices[0].message.content.strip()\n",
    "        responses.append(generated_text)\n",
    "    return responses\n",
    "\n",
    "def construct_bio_prompt(text, entities, tokens):\n",
    "    prompt = f\"Sentence has been pre-tokenized into words. Do NOT tokenize it again. Use the given tokens exactly as they are and perform BIO tagging.\\n\"\n",
    "    prompt += f\"Sentence: {text}\\n\"\n",
    "    prompt += \"Tokens: \" + \" \".join(tokens) + \"\\n\\n\"\n",
    "    prompt += \"BIO Tagging Rules:\\n\"\n",
    "    prompt += \"- B-ASP: Beginning of an Aspect term\\n\"\n",
    "    prompt += \"- I-ASP: Inside of an Aspect term\\n\"\n",
    "    prompt += \"- B-OPI: Beginning of an Opinion term\\n\"\n",
    "    prompt += \"- I-OPI: Inside of an Opinion term\\n\"\n",
    "    prompt += \"- B-LOC: Beginning of an Location term\\n\"\n",
    "    prompt += \"- I-LOC: Inside of an Location term\\n\"\n",
    "    prompt += \"- B-PLC: Beginning of an Place term\\n\"\n",
    "    prompt += \"- I-PLC: Inside of an Place term\\n\"\n",
    "    prompt += \"- O: Not related to Aspect or Opinion\\n\\n\"\n",
    "    prompt += \"Aspect refers to the attribute or feature of an entity, and Opinion indicates a subjective evaluation of that aspect.\\n\"\n",
    "    prompt += \"The following entities are present in this sentence:\\n\"\n",
    "    for ent in entities:\n",
    "        prompt += f\"- {ent['entity_name']} ({ent['class_name']})\\n\"\n",
    "    prompt += \"Even if the sentence does not contain these entities, other words can be tagged as Aspect or Opinion terms.\\n\"\n",
    "    prompt += \"Do NOT split or merge the tokens. Use them exactly as given and ensure the number of BIO tags matches the number of tokens.\\n\"\n",
    "    prompt += \"If the number of tokens and BIO tags do not match, return 'ERROR'.\\n\\n\"\n",
    "    prompt += \"Here is an example:\\n\"\n",
    "    prompt += \"Example sentence: '이 카페는 공간이 작지만, 가족친화적인 분위기가 정말 좋아요.'\\n\"\n",
    "    prompt += \"Tokens: ['이', '카페는', '공간이', '작지만', ',', '가족친화적인', '분위기가', '정말', '좋아요', '.']\\n\"\n",
    "    prompt += \"BIO Tagging:\\n\"\n",
    "    prompt += \"- Output each token with its corresponding BIO tag, separated by a slash (/).\\n\"\n",
    "    prompt += \"- Example output: '이/O 카페는/O 공간이/B-ASP 작지만/B-OPI ,/O 가족친화적인/B-ASP 분위기가/I-ASP 정말/O 좋아요/B-OPI ./O'\\n\"\n",
    "    prompt += \"- Each token must have exactly one tag.\\n\\n\"\n",
    "    prompt += \"IMPORTANT: Write the result in Korean only. No explanations or comments.\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 단어별 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = re.findall(r'\\w+|[^\\w\\s]', sentence)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub(r'\\s+', '', token)  # 공백 제거\n",
    "        token = re.sub(r'[^\\w가-힣]', '', token)  # 특수문자 제거\n",
    "        if token:\n",
    "            cleaned_tokens.append(token)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1. Data Load\n",
    "\n",
    "4단계 새로 생성 안하고 불러와서 진행할 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"03_generated_sentences.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    generated_sentences = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.2. 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [13:33<00:00,  8.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "bio_data = []\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "for item in tqdm(range(0, len(generated_sentences), batch_size)):\n",
    "    batch = generated_sentences[item:item+batch_size]\n",
    "    batch_prompts = []\n",
    "    batch_tokens = []\n",
    "\n",
    "    for i in batch:\n",
    "        tokens = tokenize_sentence(i[\"sentence\"])\n",
    "        batch_tokens.append(tokens)\n",
    "\n",
    "        prompt = construct_bio_prompt(i[\"sentence\"], i[\"entities\"], tokens)\n",
    "        batch_prompts.append(prompt)\n",
    "\n",
    "    batch_results = generate_bio_prompt(batch_prompts, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    for i, tokens, bio_result in zip(batch, batch_tokens, batch_results):\n",
    "        bio_data.append({\n",
    "            \"sentence\": i[\"sentence\"],\n",
    "            \"tokens\": tokens,\n",
    "            \"bio_tagging\": bio_result\n",
    "        })\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"04_bio_tagged_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(bio_data, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train and Test set split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Data Load\n",
    "\n",
    "로드하고 작업할 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"04_bio_tagged_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    bio_data = json.load(f)\n",
    "\n",
    "print(len(bio_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 500, Processed: 500, Errors: 0\n"
     ]
    }
   ],
   "source": [
    "check_list = []\n",
    "processed_list = []\n",
    "\n",
    "def extract_tags(tagged_output, tokens):\n",
    "    tagged_tokens = tagged_output.split()\n",
    "    fixed_tagging = []\n",
    "    \n",
    "    i, j = 0, 0\n",
    "    while i < len(tagged_tokens) and j < len(tokens):\n",
    "        current_token, current_tag = tagged_tokens[i].rsplit(\"/\", 1)\n",
    "        actual_token = tokens[j]\n",
    "        \n",
    "        if current_token ==  actual_token:\n",
    "            fixed_tagging.append(current_tag)\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            if i + 1 < len(tagged_tokens):\n",
    "                next_token, _ = tagged_tokens[i+1].rsplit(\"/\", 1)\n",
    "                combined_token = current_token + next_token\n",
    "\n",
    "                if combined_token == actual_token:\n",
    "                    fixed_tagging.append(current_tag)\n",
    "                    i += 2\n",
    "                    j += 1\n",
    "                    continue\n",
    "            print(f\"ERROR: Token mismatch - Expected: {actual_token}, Actual: {current_token}\")\n",
    "            return \"ERROR\"\n",
    "    \n",
    "    if len(fixed_tagging) != len(tokens):\n",
    "        print(f\"Length Mismatch: Tokens = {len(tokens)}, Tags = {len(fixed_tagging)}\")\n",
    "        return \"ERROR\"\n",
    "    return fixed_tagging\n",
    "\n",
    "for idx, item in enumerate(bio_data):\n",
    "    tokens = item[\"tokens\"]\n",
    "    bio_tagging = item[\"bio_tagging\"]\n",
    "    bio_tag_list = extract_tags(bio_tagging, tokens)\n",
    "\n",
    "    item[\"bio_tagging\"] = bio_tag_list\n",
    "\n",
    "    if bio_tag_list == \"ERROR\":\n",
    "        check_list.append({\n",
    "            \"sentence\": item[\"sentence\"],\n",
    "            \"tokens\": tokens,\n",
    "            \"bio_tagging\": bio_tagging,\n",
    "            \"token_length\": len(tokens),\n",
    "            \"bio_tagging_length\": len(bio_tag_list)\n",
    "        })\n",
    "    else:\n",
    "        processed_list.append(item)\n",
    "\n",
    "for item in processed_list:\n",
    "    bio_tagged = item[\"bio_tagging\"]\n",
    "    tokens = item[\"tokens\"]\n",
    "\n",
    "    if len(bio_tagged) != len(tokens):\n",
    "        print(f\"ERROR: Length mismatch - Tokens: {len(tokens)}, Tags: {len(bio_tagged)}\")\n",
    "        print(f\"Sentence: {item['sentence']}\")\n",
    "\n",
    "print(f\"Total: {len(bio_data)}, Processed: {len(processed_list)}, Errors: {len(check_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"05_processed_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(processed_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(\"06_check_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(check_list, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.2. Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 400, Test: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(processed_list, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train: {len(train_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "with open(\"../train_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "with open(\"../test_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_data, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "013a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
