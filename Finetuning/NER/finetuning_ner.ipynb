{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "monologg 코드를 따라가보는 NER finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "require\n",
    "\n",
    "attrdict\n",
    "transformer\n",
    "torch\n",
    "fastprogress\n",
    "seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from seqeval import metrics as seqeval_metrics\n",
    "\n",
    "from transformers import BertConfig, ElectraConfig\n",
    "from transformers import ElectraTokenizer\n",
    "from transformers import BertForTokenClassification, ElectraForTokenClassification\n",
    "\n",
    "def init_logger():\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if not args.no_cuda and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "CONFIG_CLASSES = {\n",
    "    \"kobert\": BertConfig,\n",
    "    \"koelectra-base\": ElectraConfig,\n",
    "    \"koelectra-small\": ElectraConfig,\n",
    "    \"koelectra-base-v2\": ElectraConfig,\n",
    "    \"koelectra-base-v3\": ElectraConfig,\n",
    "    \"koelectra-small-v2\": ElectraConfig,\n",
    "    \"koelectra-small-v3\": ElectraConfig,\n",
    "}\n",
    "\n",
    "TOKENIZER_CLASSES = {\n",
    "    # \"kobert\": KoBertTokenizer,\n",
    "    \"koelectra-base\": ElectraTokenizer,\n",
    "    \"koelectra-small\": ElectraTokenizer,\n",
    "    \"koelectra-base-v2\": ElectraTokenizer,\n",
    "    \"koelectra-base-v3\": ElectraTokenizer,\n",
    "    \"koelectra-small-v2\": ElectraTokenizer,\n",
    "    \"koelectra-small-v3\": ElectraTokenizer,\n",
    "}\n",
    "\n",
    "MODEL_FOR_TOKEN_CLASSIFICATION = {\n",
    "    \"kobert\": BertForTokenClassification,\n",
    "    \"koelectra-base\": ElectraForTokenClassification,\n",
    "    \"koelectra-small\": ElectraForTokenClassification,\n",
    "    \"koelectra-base-v2\": ElectraForTokenClassification,\n",
    "    \"koelectra-base-v3\": ElectraForTokenClassification,\n",
    "    \"koelectra-small-v2\": ElectraForTokenClassification,\n",
    "    \"koelectra-small-v3\": ElectraForTokenClassification,\n",
    "    \"koelectra-small-v3-51000\": ElectraForTokenClassification,\n",
    "}\n",
    "\n",
    "\n",
    "def simple_accuracy(labels, preds):\n",
    "    return (labels == preds).mean()\n",
    "\n",
    "\n",
    "def acc_score(labels, preds):\n",
    "    return {\n",
    "        \"acc\": simple_accuracy(labels, preds),\n",
    "    }\n",
    "\n",
    "def f1_pre_rec(labels, preds):\n",
    "    return {\n",
    "        \"precision\": seqeval_metrics.precision_score(labels, preds, suffix=True),\n",
    "        \"recall\": seqeval_metrics.recall_score(labels, preds, suffix=True),\n",
    "        \"f1\": seqeval_metrics.f1_score(labels, preds, suffix=True),\n",
    "    }\n",
    "\n",
    "def show_ner_report(labels, preds):\n",
    "    return seqeval_metrics.classification_report(labels, preds, suffix=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"\n",
    "    A single training/test example for simple sequence classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, token_type_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.label_ids = label_ids\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "\n",
    "def convert_examples_to_features(\n",
    "        args,\n",
    "        examples,\n",
    "        tokenizer,\n",
    "        max_seq_length,\n",
    "        pad_token_label_id=-100,\n",
    "):\n",
    "    label_lst = TourNerProcessor(args).get_labels() # get_labels() -> [\"O\", \"B-ASP\", \"I-ASP\", ...]\n",
    "    label_map = {label: i for i, label in enumerate(label_lst)} # {\"O\": 0, \"B-ASP\": 1, \"I-ASP\": 2, ...}\n",
    "\n",
    "    features = []\n",
    "\n",
    "    # example단위 반복\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "\n",
    "        # 10000개 단위 로그 출력\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example {} of {}\".format(ex_index, len(examples)))\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "\n",
    "        # 문장의 단어들을 학습을 위해 토크나이징\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                word_tokens = [tokenizer.unk_token]  # For handling the bad-encoded word\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        special_tokens_count = 2\n",
    "        # 최대 길이를 넘어가는 경우, max_seq_length - special_tokens_count 만큼 자름\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[:(max_seq_length - special_tokens_count)]\n",
    "\n",
    "        # Add [SEP]\n",
    "        tokens += [tokenizer.sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "\n",
    "        # Add [CLS]\n",
    "        tokens = [tokenizer.cls_token] + tokens\n",
    "        label_ids = [pad_token_label_id] + label_ids\n",
    "\n",
    "        token_type_ids = [0] * len(tokens)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "        attention_mask += [0] * padding_length\n",
    "        token_type_ids += [0] * padding_length\n",
    "        label_ids += [pad_token_label_id] * padding_length\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(attention_mask) == max_seq_length\n",
    "        assert len(token_type_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % example.guid)\n",
    "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
    "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
    "            logger.info(\"label: %s \" % \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          token_type_ids=token_type_ids,\n",
    "                          label_ids=label_ids)\n",
    "        )\n",
    "    return features\n",
    "\n",
    "class TourNerProcessor(object):\n",
    "    \"\"\"Processor for the Tour NER data set \"\"\"\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [\"O\", \"B-ASP\", \"I-ASP\", \"B-OPI\", \"I-OPI\", \"B-LOC\", \"I-LOC\", \"B-PLC\", \"I-PLC\"]\n",
    "\n",
    "    @classmethod\n",
    "    def _read_file(cls, input_file):\n",
    "        \"\"\"Read JSON file, and return a list of dictionaries (words & labels)\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "\n",
    "    def _create_examples(self, dataset, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, data) in enumerate(dataset):\n",
    "            words = data['tokens']\n",
    "            labels = data['bio_tagging']\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "\n",
    "            assert len(words) == len(labels)\n",
    "\n",
    "            if i % 10000 == 0:\n",
    "                logger.info(data)\n",
    "            examples.append(InputExample(guid=guid, words=words, labels=labels))\n",
    "        return examples\n",
    "\n",
    "    def get_examples(self, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mode: train, dev, test\n",
    "        \"\"\"\n",
    "        file_to_read = None\n",
    "        if mode == 'train':\n",
    "            file_to_read = self.args.train_file\n",
    "        elif mode == 'dev':\n",
    "            file_to_read = self.args.dev_file\n",
    "        elif mode == 'test':\n",
    "            file_to_read = self.args.test_file\n",
    "\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(self.args.data_dir,\n",
    "                                                        file_to_read)))\n",
    "        return self._create_examples(self._read_file(os.path.join(self.args.data_dir,\n",
    "                                                                  file_to_read)), mode)\n",
    "\n",
    "def load_and_cache_examples(args, tokenizer, mode):\n",
    "    # 1. NER Processor init\n",
    "    processor = TourNerProcessor(args)\n",
    "    # 2. Set the cached file path\n",
    "    cached_features_file = os.path.join(\n",
    "        args.data_dir,\n",
    "        \"cached_{}_{}_{}\".format(\n",
    "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
    "            str(args.max_seq_len),\n",
    "            mode\n",
    "        )\n",
    "        # e.g., cached_kobert_128_train\n",
    "    )\n",
    "    # 3. If the cached file exists, load it : examples\n",
    "    if os.path.exists(cached_features_file):\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file, weights_only=False)\n",
    "    # 4. If the cached file does not exist, create it : examples\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        if mode == \"train\":\n",
    "            examples = processor.get_examples(\"train\")\n",
    "        elif mode == \"dev\":\n",
    "            examples = processor.get_examples(\"dev\")\n",
    "        elif mode == \"test\":\n",
    "            examples = processor.get_examples(\"test\")\n",
    "        else:\n",
    "            raise ValueError(\"For mode, only train, dev, test is avaiable\")\n",
    "        # 5. convert examples to features\n",
    "        pad_token_label_id = CrossEntropyLoss().ignore_index # ignore padded token\n",
    "        features = convert_examples_to_features(\n",
    "            args,\n",
    "            examples,\n",
    "            tokenizer,\n",
    "            max_seq_length=args.max_seq_len,\n",
    "            pad_token_label_id=pad_token_label_id\n",
    "        )\n",
    "        # 6. Save the features into the cached file\n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    # 7. Convert features to tensor dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
    "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    # 8. Return the tensor dataset\n",
    "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_label_ids)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from attrdict import AttrDict\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "proj_root = Path.cwd().parent.parent\n",
    "\n",
    "tasks_num_labels = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, eval_dataset, mode, global_step=None):\n",
    "    results = {}\n",
    "\n",
    "    # 1. Set Eval DataLoeader\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "    # 2. Logging eval info\n",
    "    if global_step != None:\n",
    "        logger.info(\"***** Running evaluation on {} dataset ({} step) *****\".format(mode, global_step))\n",
    "    else:\n",
    "        logger.info(\"***** Running evaluation on {} dataset *****\".format(mode))\n",
    "    logger.info(\"  Num examples = {}\".format(len(eval_dataset)))\n",
    "    logger.info(\"  Eval Batch size = {}\".format(args.eval_batch_size))\n",
    "\n",
    "    # 3. Init eval loss, preds\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    # 4. Start eval\n",
    "    for batch in progress_bar(eval_dataloader):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "        # 4.1. Generate predictions\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"labels\": batch[3]\n",
    "            }\n",
    "            if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
    "                inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        # 4.2. Save results\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "    # 5. Compute loss & metrics\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    results = {\n",
    "        \"loss\": eval_loss\n",
    "    }\n",
    "    preds = np.argmax(preds, axis=2)\n",
    "\n",
    "    # 6. Mapping NER label\n",
    "    labels = [\"O\", \"B-ASP\", \"I-ASP\", \"B-OPI\", \"I-OPI\", \"B-LOC\", \"I-LOC\", \"B-PLC\", \"I-PLC\"]\n",
    "\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    # 7. convert preds, out_label_ids to list\n",
    "    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "    pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "\n",
    "    for i in range(out_label_ids.shape[0]):\n",
    "        for j in range(out_label_ids.shape[1]):\n",
    "            if out_label_ids[i, j] != pad_token_label_id:\n",
    "                out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "     \n",
    "    # 8. Compute metrics\n",
    "    result = f1_pre_rec(out_label_list, preds_list)\n",
    "    results.update(result)\n",
    "\n",
    "    # 9. Save results\n",
    "    output_dir = os.path.join(args.output_dir, mode)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    output_eval_file = os.path.join(output_dir, \"{}-{}.txt\".format(mode, global_step) if global_step else \"{}.txt\".format(mode))\n",
    "    with open(output_eval_file, \"w\") as f_w:\n",
    "        logger.info(\"***** Eval results on {} dataset *****\".format(mode))\n",
    "        for key in sorted(results.keys()):\n",
    "            logger.info(\"  {} = {}\".format(key, str(results[key])))\n",
    "            f_w.write(\"  {} = {}\\n\".format(key, str(results[key])))\n",
    "        logger.info(\"\\n\" + show_ner_report(out_label_list, preds_list)) # Show report for each tag result\n",
    "        f_w.write(\"\\n\" + show_ner_report(out_label_list, preds_list))\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args,\n",
    "          model,\n",
    "          train_dataset,\n",
    "          dev_dataset=None,\n",
    "          test_dataset=None):\n",
    "    # 1. Prepare Training\n",
    "    \n",
    "    # 1.1. Create DataLoader\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "    \n",
    "    # 1.2. Set the training steps\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    # 2. Prepare Optimizer and Scheduler\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total)\n",
    "\n",
    "    # 3. Load past checkpoint\n",
    "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
    "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
    "    ):\n",
    "        # Load optimizer and scheduler states\n",
    "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
    "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
    "\n",
    "    # 4. Training Loop\n",
    "\n",
    "    # 4.1. Setting\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Total train batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    logger.info(\"  Logging steps = %d\", args.logging_steps)\n",
    "    logger.info(\"  Save steps = %d\", args.save_steps)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss = 0.0\n",
    "\n",
    "    model.zero_grad()\n",
    "    mb = master_bar(range(int(args.num_train_epochs)))\n",
    "\n",
    "    # 4.2. Epoch\n",
    "    for epoch in mb:\n",
    "        epoch_iterator = progress_bar(train_dataloader, parent=mb)\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            # 4.3. Train mini-batch\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"labels\": batch[3]\n",
    "            }\n",
    "            if args.model_type not in [\"distilkobert\", \"xlm-roberta\"]:\n",
    "                inputs[\"token_type_ids\"] = batch[2]  # Distilkobert, XLM-Roberta don't use segment_ids\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # 4.4. Calculate gradient\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            # 4.5. Backward and Update Gradient\n",
    "            loss.backward()\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0 or (\n",
    "                    len(train_dataloader) <= args.gradient_accumulation_steps\n",
    "                    and (step + 1) == len(train_dataloader)\n",
    "            ):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                # 5. Evaluation and Save Checkpoint\n",
    "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    if args.evaluate_test_during_training:\n",
    "                        evaluate(args, model, test_dataset, \"test\", global_step)\n",
    "                    else:\n",
    "                        evaluate(args, model, dev_dataset, \"dev\", global_step)\n",
    "\n",
    "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )\n",
    "                    print(f\"Model Type: {type(model_to_save)}\")\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "\n",
    "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    logger.info(\"Saving model checkpoint to {}\".format(output_dir))\n",
    "\n",
    "                    if args.save_optimizer:\n",
    "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                        logger.info(\"Saving optimizer and scheduler states to {}\".format(output_dir))\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                break\n",
    "\n",
    "        mb.write(\"Epoch {} done\".format(epoch + 1))\n",
    "\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            break\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Config\n",
    "config_file = \"config.json\"\n",
    "with open(config_file) as f:\n",
    "    args = AttrDict(json.load(f))\n",
    "logger.info(\"Training/evaluation parameters {}\".format(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Set Model and initialize\n",
    "\n",
    "# Initialize log and Set random seed\n",
    "args.output_dir = os.path.join(args.ckpt_dir, args.output_dir)\n",
    "\n",
    "init_logger()\n",
    "set_seed(args)\n",
    "\n",
    "# Initialize Processor & get labels\n",
    "processor = TourNerProcessor(args)\n",
    "labels = processor.get_labels()\n",
    "\n",
    "# Load Model Configs\n",
    "config = CONFIG_CLASSES[args.model_type].from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    num_labels=tasks_num_labels,\n",
    "    id2label={str(i): label for i, label in enumerate(labels)},\n",
    "    label2id={label: i for i, label in enumerate(labels)},\n",
    ")\n",
    "\n",
    "# load tokenizer and model\n",
    "tokenizer = TOKENIZER_CLASSES[args.model_type].from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    do_lower_case=args.do_lower_case\n",
    ")\n",
    "model = MODEL_FOR_TOKEN_CLASSIFICATION[args.model_type].from_pretrained(\n",
    "    args.model_name_or_path,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# GPU or CPU\n",
    "args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
    "model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load dataset\n",
    "args.data_dir = os.path.join(proj_root, args.data_dir, \"NER\")\n",
    "\n",
    "train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\") if os.path.join(args.data_dir, args.train_file) else None\n",
    "test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\") if os.path.join(args.data_dir, args.test_file) else None\n",
    "dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\") if args.dev_file else None\n",
    "\n",
    "if dev_dataset == None:\n",
    "    args.evaluate_test_during_training = True  # If there is no dev dataset, only use testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train Model\n",
    "if args.do_train:\n",
    "    global_step, tr_loss = train(args, model, train_dataset, dev_dataset, test_dataset)\n",
    "    logger.info(\" global_step = {}, average loss = {}\".format(global_step, tr_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Evaluate Model\n",
    "results = {}\n",
    "if args.do_eval:\n",
    "    checkpoints = list(os.path.dirname(c) for c in\n",
    "    sorted(glob.glob(args.output_dir + \"/**/\" + \"model.safetensors\", recursive=True),\n",
    "        key=lambda path_with_step: list(map(int, re.findall(r\"\\d+\", path_with_step)))[-1]))\n",
    "    # 만약 그래도 체크포인트를 못 찾으면 에러 출력      \n",
    "    if not checkpoints:\n",
    "        logger.info(\"No checkpoints found. Trying final trained model instead.\")\n",
    "        checkpoints = [os.path.join(args.output_dir, \"final_model\")]\n",
    "    \n",
    "    if not args.eval_all_checkpoints:\n",
    "        checkpoints = checkpoints[-1:]\n",
    "    else:\n",
    "        logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "        logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "    # 6. Evaluate at each checkpoint\n",
    "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "    for checkpoint in checkpoints:\n",
    "        global_step = checkpoint.split(\"-\")[-1]\n",
    "        model = MODEL_FOR_TOKEN_CLASSIFICATION[args.model_type].from_pretrained(checkpoint)\n",
    "        model.to(args.device)\n",
    "        result = evaluate(args, model, test_dataset, mode=\"test\", global_step=global_step)\n",
    "        result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
    "        results.update(result)\n",
    "        \n",
    "    # 7. Save the final result\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "    output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as f_w:\n",
    "        if len(checkpoints) > 1:\n",
    "            for key in sorted(results.keys(), key=lambda key_with_step: (\n",
    "                    \"\".join(re.findall(r'[^_]+_', key_with_step)),\n",
    "                    int(re.findall(r\"_\\d+\", key_with_step)[-1][1:])\n",
    "            )):\n",
    "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "        else:\n",
    "            for key in sorted(results.keys()):\n",
    "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "013a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
